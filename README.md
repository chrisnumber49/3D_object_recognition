# Depth Feature Extraction with Approximated Locality-constrained Linear Coding Scheme for Few-Shot Learning Object Recognition

## Description
In this thesis we introduce an image recognition method for the 3D image. The overall shape of an object can be represented as a feature which concatenates the local histograms that consist of 2 angles, azimuth and zenith in histogram of oriented normal vectors (HONV). However, the diversity of appearance caused by the change of viewpoint might cause the reduction of accuracy, to eliminate this constrain, in locality-constrained linear coding (LLC), the local features are projected into its local-coordinate system instead, in Spatial Pyramid Matching (SPM) layer, max pooling is adopted to integrate projected features in each sub-region to generate the final representation of object. Combining all methods above, the classification accuracy in our experiment achieves higher than 81% in grayscale and higher than 92% in depth space with only 30 training samples for each category. For the object detection task, we adopt the segmentation based selective search as the region proposal to avoid the computational waste from the conventional exhaustive search method.

## System Overview
We firstly apply the HONV on the depth image, the overall shape of an object can be represented as a concatenation of local feature histograms located by grid sampling which consist of azimuthal angle and zenith angle.
Following, with the extracted features, the codebook will be generated by using K-Means cluster, each cluster centroid will be represented as a codeword for the coding process.
The approximated LLC is used to project local feature histograms into its related code words, to improve the computational efﬁciency. 
K-nearest-neighbor search is firstly performed then solving a constrained least square ﬁtting problem, this drastically decreases the computational complexity and speedup the whole encoding process.
In SPM layer, we adopt max pooling as the pooling method to integrate codes in each subregion then concatenate into 1-dimensional feature vector for the discriminative model, the classification result shall be generated.

For the instance testing, we adopt selective search as the region proposal, different from traditional exhaustive search, it makes the final number of generated proposals way less by grouping adjacent regions based on color, texture, size, and shape.
The same procedural in the training stage will be applied on each Region of interest (ROI) windows to generate the representation for the independently classification, in one image, the output will contain one or multiple object confidence score of categories with its bounding box.

The system flow chart is shown as the figure.

![image](https://github.com/chrisnumber49/3D_object_recognition/blob/main/System_flow.png)

## Steps of Implementing
1. Run Codebook_HONV.ipynb with your codebook generating images under the codebook_generating directory, the codebook pkl file will be generated.
2. Move the codebook pkl file to image_encoding directory, then run Image_Encoding_HONV.ipynb with your training image, the csv file with image features will be generated.
3. Train your own discriminative model with image features
4. With the saved model, you can run 3D_Object_Rec.ipynb for the instance object detection testing.

## Experimental Result
In this experiment, bowl, flashlight and cereal box are selected as the targets to detect, we use 50 samples for the model training in each class, the figure down below shows a detection comparison with Fast R-CNN with VGG16 model, the object with the red bounding box is considered as a bowl by the system, one with the green bounding box is considered as a flashlight, and one with light blue is considered as a cereal box.

![image](https://github.com/chrisnumber49/3D_object_recognition/blob/main/Experiment%20result.PNG)

The Table shows the comparison of average precision, where Fast R-CNN* means the training samples for each class of the VGG16 model in Fast R-CNN is 500. We can see with only 50 training data, the result of Fast R-CNN with VGG16 model is a complete failure, and even we increase the number of samples tenfold to 500 for the VGG16 model, our proposed methods still outdo the Fast R-CNN by 27.6% of the mean average precision (mAP).

![image](https://github.com/chrisnumber49/3D_object_recognition/blob/main/Average%20precision.PNG)
 
